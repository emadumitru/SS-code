

File: src/main/java/org/opentripplanner/analyst/broker/Broker.java
package org.opentripplanner.analyst.broker;

import com.fasterxml.jackson.databind.ObjectMapper;
import org.glassfish.grizzly.http.server.Response;
import org.glassfish.grizzly.http.util.HttpStatus;
import org.opentripplanner.analyst.cluster.AnalystClusterRequest;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayDeque;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Deque;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * This class watches for incoming requests for work tasks, and attempts to match them to enqueued tasks.
 * It draws tasks fairly from all users, and fairly from all jobs within each user, while attempting to respect the
 * cache affinity of each worker (give it tasks on the same graph it has been working on recently).
 *
 * When no work is available, the polling functions return immediately. Workers are expected to sleep and re-poll
 * after a few tens of seconds.
 *
 * TODO if there is a backlog of work (the usual case when jobs are lined up) workers will constantly change graphs
 * We need a queue of deferred work: (job, timestamp) when a job would have fairly had its work consumed  if a worker was available.
 * Anything that survives at the head of that queue for more than e.g. one minute gets forced on a non-affinity worker.
 * Any new workers without an affinity preferentially pull work off the deferred queue.
 * Polling worker connections scan the deferred queue before ever going to the main circular queue.
 * When the deferred queue exceeds a certain size, that's when we must start more workers.
 *
 * We should distinguish between two cases:
 * 1. we were waiting for work and woke up because work became available.
 * 2. we were waiting for a consumer and woke up when one arrived.
 *
 * The first case implies that many workers should migrate toward the new work.
 *
 * Two key ideas are:
 * 1. Least recently serviced queue of jobs
 * 2. Affinity Homeostasis
 *
 * If we can constantly keep track of the ideal proportion of workers by graph (based on active queues),
 * and the true proportion of consumers by graph (based on incoming requests) then we can decide when a worker's graph
 * affinity should be ignored.
 *
 * It may also be helpful to mark jobs every time they are skipped in the LRU queue. Each time a job is serviced,
 * it is taken out of the queue and put at its end. Jobs that have not been serviced float to the top.
 */
public class Broker implements Runnable {

    // TODO catalog of recently seen consumers by affinity with IP: response.getRequest().getRemoteAddr();

    private static final Logger LOG = LoggerFactory.getLogger(Broker.class);

    private CircularList<User> users = new CircularList<>();

    private int nUndeliveredTasks = 0;

    private int nWaitingConsumers = 0; // including some that might be closed

    private int nextTaskId = 0;

    private ObjectMapper mapper = new ObjectMapper();

    /** Outstanding requests from workers for tasks, grouped by worker graph affinity. */
    Map<String, Deque<Response>> connectionsForGraph = new HashMap<>();

    // Queue of tasks to complete Delete, Enqueue etc. to avoid synchronizing all the functions

    public synchronized void enqueueTasks (QueuePath queuePath, Collection<AnalystClusterRequest> tasks) {
        LOG.debug("Queue {}", queuePath);
        // Assumes tasks are pre-validated and are all on the same user/job
        User user = findUser(queuePath.userId, true);
        Job job = user.findJob(queuePath.jobId, true);
        for (AnalystClusterRequest task : tasks) {
            task.taskId = nextTaskId++;
            job.addTask(task);
            nUndeliveredTasks += 1;
            LOG.debug("Enqueued task id {} in job {}", task.taskId, job.jobId);
        }
        // Wake up the delivery thread if it's waiting on input.
        // This wakes whatever thread called wait() while holding the monitor for this Broker object.
        notify();
    }

    /** Long poll operations are enqueued here. */
    public synchronized void registerSuspendedResponse(String graphId, Response response) {
        // The workers are not allowed to request a specific job or task, just a specific graph and queue type.
        Deque<Response> deque = connectionsForGraph.get(graphId);
        if (deque == null) {
            deque = new ArrayDeque<>();
            connectionsForGraph.put(graphId, deque);
        }
        deque.addLast(response);
        nWaitingConsumers += 1;
        // Wake up the delivery thread if it's waiting on consumers.
        // This is whatever thread called wait() while holding the monitor for this QBroker object.
        notify();
    }

    /** When we notice that a long poll connection has closed, we remove it here. */
    public synchronized boolean removeSuspendedResponse(String graphId, Response response) {
        Deque<Response> deque = connectionsForGraph.get(graphId);
        if (deque == null) {
            return false;
        }
        if (deque.remove(response)) {
            nWaitingConsumers -= 1;
            LOG.debug("Removed closed connection from queue.");
            logQueueStatus();
            return true;
        }
        return false;
    }

    private void logQueueStatus() {
        LOG.info("Status {} undelivered, {} consumers waiting.", nUndeliveredTasks, nWaitingConsumers);
    }

    /**
     * Pull the next job queue with undelivered work fairly from users and jobs.
     * Pass some of that work to a worker, blocking if necessary until there are workers available.
     */
    public synchronized void deliverTasksForOneJob () throws InterruptedException {

        // Wait until there are some undelivered tasks.
        while (nUndeliveredTasks == 0) {
            LOG.debug("Task delivery thread is going to sleep, there are no tasks waiting for delivery.");
            logQueueStatus();
            wait();
        }
        LOG.debug("Task delivery thread is awake and there are some undelivered tasks.");
        logQueueStatus();

        // Circular lists retain iteration state via their head pointers.
        Job job = null;
        while (job == null) {
            User user = users.advance();
            if (user == null) {
                LOG.error("There should always be at least one user here, because there is an undelivered task.");
            }
            job = user.jobs.advanceToElement(e -> e.visibleTasks.size() > 0);
        }

        // We have found job with some undelivered tasks. Give them to a consumer,
        // waiting until one is available even if this means ignoring graph affinity.
        LOG.debug("Task delivery thread has found undelivered tasks in job {}.", job.jobId);
        while (true) {
            while (nWaitingConsumers == 0) {
                LOG.debug("Task delivery thread is going to sleep, there are no consumers waiting.");
                // Thread will be notified when there are new incoming consumer connections.
                wait();
            }
            LOG.debug("Task delivery thread is awake, and some consumers are waiting.");
            logQueueStatus();

            // Here, we know there are some consumer connections waiting, but we don't know if they're still open.
            // First try to get a consumer with affinity for this graph
            LOG.debug("Looking for an eligible consumer, respecting graph affinity.");
            Deque<Response> deque = connectionsForGraph.get(job.graphId);
            while (deque != null && !deque.isEmpty()) {
                Response response = deque.pop();
                nWaitingConsumers -= 1;
                if (deliver(job, response)) {
                    return;
                }
            }

            // Then try to get a consumer from the graph with the most workers
            LOG.debug("No consumers with the right affinity. Looking for any consumer.");
            List<Deque<Response>> deques = new ArrayList<>(connectionsForGraph.values());
            deques.sort((d1, d2) -> Integer.compare(d2.size(), d1.size()));
            for (Deque<Response> d : deques) {
                while (!d.isEmpty()) {
                    Response response = d.pop();
                    nWaitingConsumers -= 1;
                    if (deliver(job, response)) {
                        return;
                    }
                }
            }

            // No workers were available to accept the tasks. The thread should wait on the next iteration.
            LOG.debug("No consumer was available. They all must have closed their connections.");
            if (nWaitingConsumers != 0) {
                throw new AssertionError("There should be no waiting consumers here, something is wrong.");
            }
        }

    }

    /**
     * Attempt to hand some tasks from the given job to a waiting consumer connection.
     * The write will fail if the consumer has closed the connection but it hasn't been removed from the connection
     * queue yet because the Broker methods are synchronized (the removal action is waiting to get the monitor).
     * @return whether the handoff succeeded.
     */
    public synchronized boolean deliver (Job job, Response response) {

        // Check up-front whether the connection is still open.
        if (!response.getRequest().getRequest().getConnection().isOpen()) {
            LOG.debug("Consumer connection was closed. It will be removed.");
            return false;
        }

        // Get up to N tasks from the visibleTasks deque
        List<AnalystClusterRequest> tasks = new ArrayList<>();
        while (tasks.size() < 4 && !job.visibleTasks.isEmpty()) {
            tasks.add(job.visibleTasks.poll());
        }

        // Attempt to deliver the tasks to the given consumer.
        try {
            response.setStatus(HttpStatus.OK_200);
            OutputStream out = response.getOutputStream();
            mapper.writeValue(out, tasks);
            response.resume();
        } catch (IOException e) {
            // The connection was probably closed by the consumer, but treat it as a server error.
            LOG.debug("Consumer connection caused IO error, it will be removed.");
            response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR_500);
            response.resume();
            // Delivery failed, put tasks back on (the end of) the queue.
            job.visibleTasks.addAll(tasks);
            return false;
        }

        // Delivery succeeded, move tasks from undelivered to delivered status
        LOG.debug("Delivery of {} tasks succeeded.", tasks.size());
        nUndeliveredTasks -= tasks.size();
        job.markTasksDelivered(tasks);
        return true;

    }

    /**
     * Take a task out of the job, marking it as completed. The body of this DELETE request...
     * @return whether the task was found and removed.
     */
    public synchronized boolean deleteTask (QueuePath queuePath) {

        User user = findUser(queuePath.userId, false);
        if (user == null) {
            return false;
        }

        Job job = user.findJob(queuePath.jobId, false);
        if (job == null) {
            return false;
        }

        // There could be thousands of invisible (delivered) tasks, so we use a hash map.
        // We only allow removal of invisible tasks for now.
        // Return whether removal call discovered an existing task.
        return job.invisibleTasks.remove(queuePath.taskId) != null;

    }

    // Todo: occasionally purge closed connections from connectionsForGraph

    @Override
    public void run() {
        while (true) {
            try {
                deliverTasksForOneJob();
            } catch (InterruptedException e) {
                LOG.warn("Task pump thread was interrupted.");
                return;
            }
        }
    }

    /** Search through the users to find one with the given ID, without advancing the head of the circular list. */
    public User findUser (String userId, boolean create) {
        for (User user : users) {
            if (user.userId.equals(userId)) {
                return user;
            }
        }
        if (create) {
            User user = new User(userId);
            users.insertAtTail(user);
            return user;
        }
        return null;
    }


}


File: src/main/java/org/opentripplanner/analyst/broker/BrokerHttpHandler.java
package org.opentripplanner.analyst.broker;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.glassfish.grizzly.http.Method;
import org.glassfish.grizzly.http.server.HttpHandler;
import org.glassfish.grizzly.http.server.Request;
import org.glassfish.grizzly.http.server.Response;
import org.glassfish.grizzly.http.util.HttpStatus;
import org.opentripplanner.analyst.cluster.AnalystClusterRequest;
import org.opentripplanner.api.model.AgencyAndIdSerializer;

import java.io.IOException;
import java.util.List;

/**
* A Grizzly Async Http Service (uses reponse suspend/resume)
 * https://blogs.oracle.com/oleksiys/entry/grizzly_2_0_httpserver_api1
 *
 * When resuming a response object, "The only reliable way to check the socket status is to try to read or
 * write something." Though you also have:
 *
 * response.getRequest().getRequest().getConnection().isOpen()
 * response.getRequest().getRequest().getConnection().addCloseListener();
 * But none of these work, I've tried all three of them. You can even write to the outputstream after the connection
 * is closed.
 * Solution: networkListener.getTransport().setIOStrategy(SameThreadIOStrategy.getInstance());
 * This makes all three work! isOpen, CloseListener, and IOExceptions from flush();
 *
 * Grizzly has Comet support, but this seems geared toward subscriptions to broadcast events.
 *
*/
class BrokerHttpHandler extends HttpHandler {

    // TODO we should really just make one static mapper somewhere and use it throughout OTP
    private ObjectMapper mapper = new ObjectMapper()
            .registerModule(AgencyAndIdSerializer.makeModule())
            .setSerializationInclusion(JsonInclude.Include.NON_NULL);;

    private Broker broker;

    public BrokerHttpHandler(Broker broker) {
        this.broker = broker;
    }

    @Override
    public void service(Request request, Response response) throws Exception {

        // request.getRequestURI(); // without protocol or server, only request path
        // request.getPathInfo(); // without handler base path

        response.setContentType("application/json");

        // may be a partially specified QueuePath without job or task ID
        QueuePath queuePath = new QueuePath(request.getPathInfo());

        // Request body is expected to be JSON. Rather than loading it into a string, we could parse it to a tree
        // or bind to a type immediately. However binding introduces a dependency on the message type classes.
        try {
            if (request.getMethod() == Method.HEAD) {
                /* Let the client know server is alive and URI + request are valid. */
                mapper.readTree(request.getInputStream());
                response.setStatus(HttpStatus.OK_200);
                return;
            } else if (request.getMethod() == Method.GET) {
                /* Return a chunk of tasks for a particular graph. */
                request.getRequest().getConnection().addCloseListener((closeable, iCloseType) -> {
                    broker.removeSuspendedResponse(queuePath.graphId, response);
                });
                response.suspend(); // This request should survive after the handler function exits.
                broker.registerSuspendedResponse(queuePath.graphId, response);
            } else if (request.getMethod() == Method.POST) {
                /* Enqueue new messages. */
                // Text round trip through JSON is done in the HTTP handler thread, does not block the broker thread.
                List<AnalystClusterRequest> tasks =
                    mapper.readValue(request.getInputStream(), new TypeReference<List<AnalystClusterRequest>>(){});
                for (AnalystClusterRequest task : tasks) {
                    if (!task.graphId.equals(queuePath.graphId)
                            || !task.userId.equals(queuePath.userId)
                            || !task.jobId.equals(queuePath.jobId)) {
                        response.setStatus(HttpStatus.BAD_REQUEST_400);
                        response.setDetailMessage("Task graph/user/job ID does not match POST path.");
                        return;
                    }
                }
                broker.enqueueTasks(queuePath, tasks);
                response.setStatus(HttpStatus.ACCEPTED_202);
            } else if (request.getMethod() == Method.DELETE) {
                /* Acknowledge completion of a task and remove it from queues. */
                if (broker.deleteTask(queuePath)) {
                    response.setStatus(HttpStatus.OK_200);
                } else {
                    response.setStatus(HttpStatus.NOT_FOUND_404);
                }
            } else {
                response.setStatus(HttpStatus.BAD_REQUEST_400);
                response.setDetailMessage("Unrecognized HTTP method.");
            }
        } catch (JsonProcessingException jpex) {
            response.setStatus(HttpStatus.BAD_REQUEST_400);
            response.setDetailMessage("Could not decode/encode JSON payload. " + jpex.getMessage());
            jpex.printStackTrace();
        } catch (Exception ex) {
            response.setStatus(HttpStatus.INTERNAL_SERVER_ERROR_500);
            response.setDetailMessage(ex.toString());
            ex.printStackTrace();
        }
    }

    public void writeJson (Response response, Object object) throws IOException {
        mapper.writeValue(response.getOutputStream(), object);
    }

}


File: src/main/java/org/opentripplanner/analyst/broker/BrokerMain.java
package org.opentripplanner.analyst.broker;

import org.glassfish.grizzly.http.server.HttpServer;
import org.glassfish.grizzly.http.server.NetworkListener;
import org.glassfish.grizzly.strategies.SameThreadIOStrategy;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.net.BindException;

// benchmark: $ ab -n 2000 -k -c 100 http://localhost:9001/

public class BrokerMain {

    private static final Logger LOG = LoggerFactory.getLogger(BrokerMain.class);

    private static final int PORT = 9001;

    private static final String BIND_ADDRESS = "0.0.0.0";

    public static void main(String[] args) {

        LOG.info("Starting qbroker on port {} of interface {}", PORT, BIND_ADDRESS);
        HttpServer httpServer = new HttpServer();
        NetworkListener networkListener = new NetworkListener("qbroker", BIND_ADDRESS, PORT);
        networkListener.getTransport().setIOStrategy(SameThreadIOStrategy.getInstance()); // we avoid blocking IO, and this allows us to see closed connections.
        httpServer.addListener(networkListener);
        // Bypass Jersey etc. and add a low-level Grizzly handler.
        // As in servlets, * is needed in base path to identify the "rest" of the path.
        Broker broker = new Broker();
        httpServer.getServerConfiguration().addHttpHandler(new BrokerHttpHandler(broker), "/*");
        try {
            httpServer.start();
            LOG.info("Broker running.");
            broker.run(); // run queue broker task pump in this thread
            Thread.currentThread().join();
        } catch (BindException be) {
            LOG.error("Cannot bind to port {}. Is it already in use?", PORT);
        } catch (IOException ioe) {
            LOG.error("IO exception while starting server.");
        } catch (InterruptedException ie) {
            LOG.info("Interrupted, shutting down.");
        }
        httpServer.shutdown();

    }


}


File: src/main/java/org/opentripplanner/analyst/broker/CircularList.java
package org.opentripplanner.analyst.broker;

import java.util.Iterator;
import java.util.Spliterator;
import java.util.function.Consumer;
import java.util.function.Predicate;

/**
 *
 */
public class CircularList<T> implements Iterable<T> {

    Node<T> head = null;

    public class CircularListIterator implements Iterator<T> {

        Node<T> curr = head;

        @Override
        public boolean hasNext() {
            return curr != null;
        }

        @Override
        public T next() {
            T currElement = curr.element;
            curr = curr.next;
            if (curr == head) {
                curr = null; // No more elements
            }
            return currElement;
        }
    }

    @Override
    public Iterator<T> iterator() {
        return new CircularListIterator();
    }

    @Override
    public void forEach(Consumer<? super T> action) {
        Node<T> curr = head;
        do {
            action.accept(curr.element);
            curr = curr.next;
        } while (curr != head);
    }

    @Override
    public Spliterator<T> spliterator() {
        throw new UnsupportedOperationException();
    }

    private static class Node<T> {
        Node prev;
        Node next;
        T element;
    }

    /** Insert an element at the tail of the circular list (the position just previous to the head). */
    public void insertAtTail (T element) {
        Node<T> newNode = new Node<>();
        newNode.element = element;
        if (head == null) {
            newNode.next = newNode;
            newNode.prev = newNode;
            head = newNode;
        } else {
            newNode.prev = head.prev;
            newNode.next = head;
            head.prev.next = newNode;
            head.prev = newNode;
        }
    }


    /** Insert a new element at the head of the circular list. */
    public void insertAtHead (T element) {
        // Add at tail and then back the head up one element, wrapping around to the tail.
        insertAtTail(element);
        head = head.prev;
    }

    /** Get the element at the head of the list without removing it. */
    public T peek () {
        return head.element;
    }

    /** Take an element off the head of the list, removing it from the list. */
    public T pop () {
        if (head == null) {
            return null;
        }
        T element = head.element;
        if (head == head.next) {
            // Last node consumed. List is now empty.
            head = null;
        } else {
            head.prev.next = head.next;
            head.next.prev = head.prev;
            head = head.next;
        }
        return element;
    }

    /**
     * Advance the head of the circular list forward to the next element.
     * Return the element at the head of the list, leaving that element in the list.
     */
    public T advance() {
        if (head == null) {
            return null;
        }
        T headElement = head.element;
        head = head.next;
        return headElement;
    }

    /**
     * Advances through the circular list returning the first element for which the predicate evaluates to true.
     * If there is no such element, returns null leaving the head of the list back in the same place.
     */
    public T advanceToElement (Predicate<T> predicate) {
        Node<T> start = head;
        do {
            T currElement = advance();
            if (predicate.test(currElement)) {
                return currElement;
            }
        } while (head != start);
        return null;
    }

}


File: src/main/java/org/opentripplanner/analyst/broker/Job.java
package org.opentripplanner.analyst.broker;

import gnu.trove.map.TIntLongMap;
import gnu.trove.map.TIntObjectMap;
import gnu.trove.map.hash.TIntLongHashMap;
import gnu.trove.map.hash.TIntObjectHashMap;
import org.opentripplanner.analyst.cluster.AnalystClusterRequest;

import java.util.ArrayDeque;
import java.util.List;
import java.util.Queue;

/**
 * FIXME delivered tasks map is oblivious to multiple tasks having the same ID.
 * In fact we just generate numeric queue task IDs. Origin point IDs will be handled at the application layer.
 */
public class Job {

    private int nTasks = 0;

    /* Defines cache affinity group for contained tasks. TODO set this when created. */
    String graphId;

    public final String jobId;

    /* Tasks awaiting delivery. */
    Queue<AnalystClusterRequest> visibleTasks = new ArrayDeque<>();

    /* Tasks that have been delivered to a worker but are awaiting completion. */
    TIntObjectMap<AnalystClusterRequest> invisibleTasks = new TIntObjectHashMap<>();

    TIntLongMap invisibleUntil = new TIntLongHashMap();

    public Job (String jobId) {
        this.jobId = jobId;
    }

    /** Adds a task to this Job, assigning it a task ID number. */
    public void addTask (AnalystClusterRequest task) {
        nTasks++;
        visibleTasks.add(task);
    }

    public void markTasksDelivered(List<AnalystClusterRequest> tasks) {
        long deliveryTime = System.currentTimeMillis();
        long visibleAt = deliveryTime + 60000; // one minute
        for (AnalystClusterRequest task : tasks) {
            invisibleUntil.put(task.taskId, visibleAt);
            invisibleTasks.put(task.taskId, task);
        }
    }

}


File: src/main/java/org/opentripplanner/analyst/broker/QueuePath.java
package org.opentripplanner.analyst.broker;

public class QueuePath {

    public String queueType;
    public String userId;
    public String graphId;
    public String jobId; // should be generated like task IDs
    public int taskId = -1;

    /**
     * path is /queueType/userId/graphId/jobId/taskId
     */
    public QueuePath (String uri) {
        String[] pathComponents = uri.split("/");
        // position 0 is empty because the path starts with a slash
        if (pathComponents.length > 1 && !pathComponents[1].isEmpty()) {
            queueType = pathComponents[1];
        }
        if (pathComponents.length > 2 && !pathComponents[2].isEmpty()) {
            userId = pathComponents[2];
        }
        if (pathComponents.length > 3 && !pathComponents[3].isEmpty()) {
            graphId = pathComponents[3];
        }
        if (pathComponents.length > 4 && !pathComponents[4].isEmpty()) {
            jobId = pathComponents[4];
        }
        if (pathComponents.length > 5 && !pathComponents[5].isEmpty()) {
            taskId = Integer.parseInt(pathComponents[5]);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        QueuePath queuePath = (QueuePath) o;

        if (taskId != queuePath.taskId) return false;
        if (graphId != null ? !graphId.equals(queuePath.graphId) : queuePath.graphId != null) return false;
        if (jobId != null ? !jobId.equals(queuePath.jobId) : queuePath.jobId != null) return false;
        if (queueType != null ? !queueType.equals(queuePath.queueType) : queuePath.queueType != null) return false;
        if (userId != null ? !userId.equals(queuePath.userId) : queuePath.userId != null) return false;

        return true;
    }

    @Override
    public int hashCode() {
        int result = queueType != null ? queueType.hashCode() : 0;
        result = 31 * result + (userId != null ? userId.hashCode() : 0);
        result = 31 * result + (graphId != null ? graphId.hashCode() : 0);
        result = 31 * result + (jobId != null ? jobId.hashCode() : 0);
        result = 31 * result + taskId;
        return result;
    }

    @Override
    public String toString() {
        return "QueuePath{" +
                "queueType='" + queueType + '\'' +
                ", userId='" + userId + '\'' +
                ", graphId='" + graphId + '\'' +
                ", jobId='" + jobId + '\'' +
                ", taskId=" + taskId +
                '}';
    }
}


File: src/main/java/org/opentripplanner/analyst/broker/QueueType.java
package org.opentripplanner.analyst.broker;

/**
 *
 */
public class QueueType {

    String typeId;


}


File: src/main/java/org/opentripplanner/analyst/broker/User.java
package org.opentripplanner.analyst.broker;

/**
 *
 */
public class User {

    public final String userId;
    public String region; // only worker machines in this AWS region should handle tasks for this user
    public final CircularList<Job> jobs = new CircularList<>();

    public User(String userId) {
        this.userId = userId;
    }

    public Job findJob (String jobId, boolean create) {
        for (Job job : jobs) {
            if (job.jobId.equals(jobId)) {
                return job;
            }
        }
        if (create) {
            Job job = new Job(jobId);
            jobs.insertAtTail(job);
            return job;
        }
        return null;
    }


}


File: src/main/java/org/opentripplanner/analyst/cluster/AnalystClusterRequest.java
package org.opentripplanner.analyst.cluster;

import org.opentripplanner.profile.ProfileRequest;
import org.opentripplanner.routing.core.RoutingRequest;

import java.io.Serializable;

/**
 * A request sent to an Analyst cluster worker.
 * It has two separate fields for RoutingReqeust or ProfileReqeust to facilitate binding from JSON.
 * Only one of them should be set in a given instance, with the ProfileRequest taking precedence if both are set.
 */
public class AnalystClusterRequest implements Serializable {

	/** The ID of the destinations pointset */
	public String destinationPointsetId;

	/** The Analyst Cluster user that created this request */
	public String userId;

	/** The ID of the graph against which to calculate this request */
	public String graphId;

	/** The job ID this is associated with */
	public String jobId;

	/** The id of this particular origin */
	public String id;

	/** To where should the result be POSTed */
	public String directOutputUrl;

	/** A unique identifier for this request assigned by the queue/broker system. */
	public int taskId;

	/**
	 * To what queue should the notification of the result be delivered?
	 */
	public String outputQueue;

	/**
	 * Where should the job be saved?
	 */
	public String outputLocation;

	/**
	 * The routing parameters to use if this is a one-to-many profile request.
	 * If profileRequest is provided, it will take precedence and routingRequest will be ignored.
	 */
	public ProfileRequest profileRequest;

	/**
	 * The routing parameters to use if this is a non-profile one-to-many request.
	 * If profileRequest is not provided, we will fall back on this routingRequest.
	 */
	public RoutingRequest routingRequest;

	/** Should times be included in the results (i.e. ResultSetWithTimes rather than ResultSet) */
	public boolean includeTimes = false;
	
	private AnalystClusterRequest(String destinationPointsetId, String graphId) {
		this.destinationPointsetId = destinationPointsetId;
		this.graphId = graphId;
	}

	/** Create a cluster request that wraps a ProfileRequest, and has no RoutingRequest. */
	public AnalystClusterRequest(String destinationPointsetId, String graphId, ProfileRequest req) {
		this(destinationPointsetId, graphId);
		routingRequest = null;
		try {
			profileRequest = req.clone();
		} catch (CloneNotSupportedException e) {
			throw new AssertionError();
		}
		profileRequest.analyst = true;
		profileRequest.toLat = profileRequest.fromLat;
		profileRequest.toLon = profileRequest.fromLon;
	}

	/** Create a cluster request that wraps a RoutingRequest, and has no ProfileRequest. */
	public AnalystClusterRequest(String destinationPointsetId, String graphId, RoutingRequest req) {
		this(destinationPointsetId, graphId);
		profileRequest = null;
		routingRequest = req.clone();
		routingRequest.batch = true;
		routingRequest.rctx = null;
	}

	/** Used for deserialization from JSON */
	public AnalystClusterRequest () { /* do nothing */ }
}


File: src/main/java/org/opentripplanner/analyst/cluster/AnalystWorker.java
package org.opentripplanner.analyst.cluster;

import com.amazonaws.regions.Region;
import com.amazonaws.regions.Regions;
import com.amazonaws.services.s3.AmazonS3;
import com.amazonaws.services.s3.AmazonS3Client;
import com.fasterxml.jackson.core.JsonParser;
import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.http.HttpEntity;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.HttpDelete;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.conn.HttpHostConnectException;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.BasicHttpParams;
import org.apache.http.params.HttpConnectionParams;
import org.apache.http.params.HttpParams;
import org.apache.http.util.EntityUtils;
import org.opentripplanner.analyst.PointSet;
import org.opentripplanner.analyst.ResultSet;
import org.opentripplanner.analyst.SampleSet;
import org.opentripplanner.api.model.AgencyAndIdSerializer;
import org.opentripplanner.profile.RepeatedRaptorProfileRouter;
import org.opentripplanner.routing.core.RoutingRequest;
import org.opentripplanner.routing.graph.Graph;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.io.OutputStream;
import java.io.PipedInputStream;
import java.io.PipedOutputStream;
import java.net.SocketTimeoutException;
import java.util.List;
import java.util.Random;
import java.util.zip.GZIPOutputStream;

/**
 *
 */
public class AnalystWorker implements Runnable {

    private static final Logger LOG = LoggerFactory.getLogger(AnalystWorker.class);

    public static final int POLL_TIMEOUT = 10000;

    public static final Random random = new Random();

    ObjectMapper objectMapper;

    String BROKER_BASE_URL = "http://localhost:9001";

    String s3Prefix = "analyst-dev";

    DefaultHttpClient httpClient = new DefaultHttpClient();

    // Of course this will eventually need to be shared between multiple AnalystWorker threads.
    ClusterGraphBuilder clusterGraphBuilder;

    // Of course this will eventually need to be shared between multiple AnalystWorker threads.
    PointSetDatastore pointSetDatastore;

    // Clients for communicating with Amazon web services
    AmazonS3 s3;

    String graphId = null;
    long startupTime;

    // Region awsRegion = Region.getRegion(Regions.EU_CENTRAL_1);
    Region awsRegion = Region.getRegion(Regions.US_EAST_1);

    boolean isSinglePoint = false;

    public AnalystWorker () {

        startupTime = System.currentTimeMillis() / 1000; // TODO auto-shutdown

        // When creating the S3 and SQS clients use the default credentials chain.
        // This will check environment variables and ~/.aws/credentials first, then fall back on
        // the auto-assigned IAM role if this code is running on an EC2 instance.
        // http://docs.aws.amazon.com/AWSSdkDocsJava/latest/DeveloperGuide/java-dg-roles.html
        s3 = new AmazonS3Client();
        s3.setRegion(awsRegion);

        /* The ObjectMapper (de)serializes JSON. */
        objectMapper = new ObjectMapper();
        objectMapper.configure(JsonParser.Feature.ALLOW_COMMENTS, true);
        objectMapper.configure(JsonParser.Feature.ALLOW_UNQUOTED_FIELD_NAMES, true);
        objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false); // ignore JSON fields that don't match target type

        /* Tell Jackson how to (de)serialize AgencyAndIds, which appear as map keys in routing requests. */
        objectMapper.registerModule(AgencyAndIdSerializer.makeModule());

        /* These serve as lazy-loading caches for graphs and point sets. */
        clusterGraphBuilder = new ClusterGraphBuilder(s3Prefix + "-graphs");
        pointSetDatastore = new PointSetDatastore(10, null, false, s3Prefix + "-pointsets");

        /* The HTTP Client for talking to the Analyst Broker. */
        HttpParams httpParams = new BasicHttpParams();
        HttpConnectionParams.setConnectionTimeout(httpParams, POLL_TIMEOUT);
        HttpConnectionParams.setSoTimeout(httpParams, POLL_TIMEOUT);
        HttpConnectionParams.setSoKeepalive(httpParams, true);
        httpClient.setParams(httpParams);

    }

    @Override
    public void run() {
        // Loop forever, attempting to fetch some messages from a queue and process them.
        while (true) {
            LOG.info("Long-polling for work ({} second timeout).", POLL_TIMEOUT/1000.0);
            // Long-poll (wait a few seconds for messages to become available)
            // TODO internal blocking queue feeding work threads, polls whenever queue.size() < nProcessors
            List<AnalystClusterRequest> tasks = getSomeWork();
            if (tasks == null) {
                LOG.info("Didn't get any work. Retrying.");
                continue;
            }
            tasks.parallelStream().forEach(this::handleOneRequest);
            // Remove messages from queue so they won't be re-delivered to other workers.
            LOG.info("Removing requests from broker queue.");
            for (AnalystClusterRequest task : tasks) {
                boolean success = deleteRequest(task);
                LOG.info("deleted task {}: {}", task.taskId, success ? "SUCCESS" : "FAIL");
            }
        }
    }

    private void handleOneRequest(AnalystClusterRequest clusterRequest) {
        try {
            LOG.info("Handling message {}", clusterRequest.toString());

            // Get the graph object for the ID given in the request, fetching inputs and building as needed.
            // All requests handled together are for the same graph, and this call is synchronized so the graph will
            // only be built once.
            Graph graph = clusterGraphBuilder.getGraph(clusterRequest.graphId);
            graphId = clusterRequest.graphId; // Record graphId so we "stick" to this same graph on subsequent polls

            // This result envelope will hold the result of the profile or single-time one-to-many search.
            ResultEnvelope envelope = new ResultEnvelope();
            if (clusterRequest.profileRequest != null) {
                // TODO check graph and job ID against queue URL for coherency
                SampleSet sampleSet = null;
                if (clusterRequest.destinationPointsetId != null) {
                    // A pointset was specified, calculate travel times to the points in the pointset.
                    // Fetch the set of points we will use as destinations for this one-to-many search
                    PointSet pointSet = pointSetDatastore.get(clusterRequest.destinationPointsetId);
                    sampleSet = pointSet.getSampleSet(graph);
                }
                // Passing a null SampleSet parameter will properly return only isochrones in the RangeSet
                RepeatedRaptorProfileRouter router =
                        new RepeatedRaptorProfileRouter(graph, clusterRequest.profileRequest, sampleSet);
                router.route();
                ResultSet.RangeSet results = router.makeResults(clusterRequest.includeTimes);
                // put in constructor?
                envelope.bestCase  = results.min;
                envelope.avgCase   = results.avg;
                envelope.worstCase = results.max;
            } else {
                // No profile request, this must be a plain one to many routing request.
                RoutingRequest routingRequest = clusterRequest.routingRequest;
                // TODO finish the non-profile case
            }

            if (clusterRequest.outputQueue != null) {
                // TODO Enqueue a notification that the work is done
            }
            if (clusterRequest.outputLocation != null) {
                // Convert the result envelope and its contents to JSON and gzip it in this thread.
                // Transfer the results to Amazon S3 in another thread, piping between the two.
                try {
                    String s3key = String.join("/", clusterRequest.jobId, clusterRequest.id + ".json.gz");
                    PipedInputStream inPipe = new PipedInputStream();
                    PipedOutputStream outPipe = new PipedOutputStream(inPipe);
                    new Thread(() -> {
                        s3.putObject(clusterRequest.outputLocation, s3key, inPipe, null);
                    }).start();
                    OutputStream gzipOutputStream = new GZIPOutputStream(outPipe);
                    objectMapper.writeValue(gzipOutputStream, envelope);
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }

        } catch (Exception ex) {
            LOG.error("An error occurred while routing: " + ex.getMessage());
            ex.printStackTrace();
        }

    }

    public List<AnalystClusterRequest> getSomeWork() {

        // Run a GET request (long-polling for work)
        String url = BROKER_BASE_URL + "/jobs/userA/graphA/jobA";
        HttpGet httpGet = new HttpGet(url);
        HttpResponse response = null;
        try {
            response = httpClient.execute(httpGet);
            if (response.getStatusLine().getStatusCode() != 200) {
                return null;
            }
            HttpEntity entity = response.getEntity();
            if (entity == null) {
                return null;
            }
            return objectMapper.readValue(entity.getContent(), new TypeReference<List<AnalystClusterRequest>>(){});
        } catch (JsonProcessingException e) {
            LOG.error("JSON processing exception while getting work: {}", e.getMessage());
        } catch (SocketTimeoutException stex) {
            LOG.error("Socket timeout while waiting to receive work.");
        } catch (HttpHostConnectException ce) {
            LOG.error("Broker refused connection. Sleeping before retry.");
            try {
                Thread.currentThread().sleep(5000);
            } catch (InterruptedException e) { }
        } catch (IOException e) {
            LOG.error("IO exception while getting work.");
            e.printStackTrace();
        }
        return null;

    }

    /** DELETE the given message from the broker, indicating that it has been processed by a worker. */
    public boolean deleteRequest (AnalystClusterRequest clusterRequest) {
        String url = BROKER_BASE_URL + String.format("/jobs/%s/%s/%s/%s", clusterRequest.userId, clusterRequest.graphId, clusterRequest.jobId, clusterRequest.taskId);
        HttpDelete httpDelete = new HttpDelete(url);
        try {
            // TODO provide any parse errors etc. that occurred on the worker as the request body.
            HttpResponse response = httpClient.execute(httpDelete);
            // Signal the http client that we're done with this response, allowing connection reuse.
            EntityUtils.consumeQuietly(response.getEntity());
            return (response.getStatusLine().getStatusCode() == 200);
        } catch (Exception e) {
            e.printStackTrace();
            return false;
        }
    }

}

File: src/main/java/org/opentripplanner/analyst/cluster/ClusterGraphService.java
package org.opentripplanner.analyst.cluster;

import com.amazonaws.AmazonServiceException;
import com.amazonaws.auth.AWSCredentials;
import com.amazonaws.auth.profile.ProfileCredentialsProvider;
import com.amazonaws.services.s3.AmazonS3Client;
import com.amazonaws.services.s3.model.S3Object;
import com.google.common.collect.Maps;
import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;
import org.opentripplanner.graph_builder.GraphBuilder;
import org.opentripplanner.routing.graph.Graph;
import org.opentripplanner.routing.impl.DefaultStreetVertexIndexFactory;
import org.opentripplanner.routing.services.GraphService;
import org.opentripplanner.routing.services.GraphSource;
import org.opentripplanner.routing.services.GraphSource.Factory;
import org.opentripplanner.standalone.CommandLineParameters;
import org.opentripplanner.standalone.Router;

import java.io.*;
import java.util.Collection;
import java.util.Enumeration;
import java.util.Map;
import java.util.zip.ZipEntry;
import java.util.zip.ZipException;
import java.util.zip.ZipFile;
import java.util.zip.ZipOutputStream;

// TODO does not really need to extend GraphService
public class ClusterGraphService extends GraphService { 

	static File GRAPH_DIR = new File("cache", "graphs");
	
	private String graphBucket;
	
	private Boolean workOffline = false;
	private AmazonS3Client s3;

	// don't use more than 60% of free memory to cache graphs
	private Map<String,Router> graphMap = Maps.newConcurrentMap();
	
	@Override
	public synchronized Router getRouter(String graphId) {
		
		GRAPH_DIR.mkdirs();
		
		if(!graphMap.containsKey(graphId)) {
			
			try {
				if (!bucketCached(graphId)) {
					if(!workOffline) {
						downloadGraphSourceFiles(graphId, GRAPH_DIR);
					}
				}
			} catch (IOException e) {
				e.printStackTrace();
			}
			
			CommandLineParameters params = new CommandLineParameters();
			params.build = new File(GRAPH_DIR, graphId);
			params.inMemory = true;
			GraphBuilder gbt = GraphBuilder.forDirectory(params, params.build);
			gbt.run();
			
			Graph g = gbt.getGraph();
			
			g.routerId = graphId;
			
			g.index(new DefaultStreetVertexIndexFactory());

			g.index.clusterStopsAsNeeded();
			
			Router r = new Router(graphId, g);
			
			// temporarily disable graph caching so we don't run out of RAM.
			// Long-term we will use an actual cache for this.
			//graphMap.put(graphId,r);
			
			return r;
					
		}
		else {
			return graphMap.get(graphId);
		}
	}

	public ClusterGraphService(String s3CredentialsFilename, Boolean workOffline, String bucket) {
		
		if(!workOffline) {
			if (s3CredentialsFilename != null) {
				AWSCredentials creds = new ProfileCredentialsProvider(s3CredentialsFilename, "default").getCredentials();
				s3 = new AmazonS3Client(creds);
			}
			else {
				// This will first check for credentials in environment variables or ~/.aws/credentials
				// then fall back on S3 credentials propagated to EC2 instances via IAM roles.
				s3 = new AmazonS3Client();
			}
			
			this.graphBucket = bucket;
		}
		
		this.workOffline = workOffline;
	}
	
	// adds either a zip file or graph directory to S3, or local cache for offline use
	public void addGraphFile(File graphFile) throws IOException {
		
		String graphId = graphFile.getName();
		
		if(graphId.endsWith(".zip"))
			graphId = graphId.substring(0, graphId.length() - 4);
		
		File graphDir = new File(GRAPH_DIR, graphId);
		
		if (graphDir.exists()) {
			if (graphDir.list().length == 0) {
				graphDir.delete();
			}
			else {
				return;
			}
		}
		
		// if we're here the directory has either been deleted or never existed
		graphDir.mkdirs();
		
		File graphDataZip = new File(GRAPH_DIR, graphId + ".zip");
				
		// if directory zip contents  store as zip
		// either way ensure there is an extracted copy in the local cache
		if(graphFile.isDirectory()) {
			FileUtils.copyDirectory(graphFile, graphDir);
			
			zipGraphDir(graphDir, graphDataZip);
		}
		else if(graphFile.getName().endsWith(".zip")) {
			FileUtils.copyFile(graphFile, graphDataZip);
			unpackGraphZip(graphDataZip, graphDir, false);
		}
		else {
			graphDataZip = null;
		}
			
		if(!workOffline && graphDataZip != null) {
			// only upload if it's not there already
			try {
				s3.getObject(graphBucket, graphId + ".zip");
			} catch (AmazonServiceException e) {
				s3.putObject(graphBucket, graphId+".zip", graphDataZip);
			}
		}
		
		graphDataZip.delete();
		
	}
	
	public synchronized File getZippedGraph(String graphId) throws IOException {
		
		File graphDataDir = new File(GRAPH_DIR, graphId);
		
		File graphZipFile = new File(GRAPH_DIR, graphId + ".zip");
		
		if(!graphDataDir.exists() && graphDataDir.isDirectory()) {
			
			FileOutputStream fileOutputStream = new FileOutputStream(graphZipFile);
			ZipOutputStream zipOutputStream = new ZipOutputStream(fileOutputStream);
			
			byte[] buffer = new byte[1024];
			
			for(File f : graphDataDir.listFiles()) {
				ZipEntry zipEntry = new ZipEntry(f.getName());
				zipOutputStream.putNextEntry(zipEntry);
	    		FileInputStream fileInput = new FileInputStream(f);

	    		int len;
	    		while ((len = fileInput.read(buffer)) > 0) {
	    			zipOutputStream.write(buffer, 0, len);
	    		}
	 
	    		fileInput.close();
	    		zipOutputStream.closeEntry();
			}
			
			zipOutputStream.close();
			
			return graphZipFile;
					
		}
		
		return null;
		
	}
	
	private static boolean bucketCached(String graphId) throws IOException {
		File graphData = new File(GRAPH_DIR, graphId);
		
		// check if cached but only as zip
		if(!graphData.exists()) {
			File graphDataZip = new File(GRAPH_DIR, graphId + ".zip");
			
			if(graphDataZip.exists()) {
				zipGraphDir(graphData, graphDataZip);
			}
		}
		
		
		return graphData.exists() && graphData.isDirectory();
	}

	private void downloadGraphSourceFiles(String graphId, File dir) throws IOException {

		File graphCacheDir = dir;
		if (!graphCacheDir.exists())
			graphCacheDir.mkdirs();

		File graphZipFile = new File(graphCacheDir, graphId + ".zip");

		File extractedGraphDir = new File(graphCacheDir, graphId);

		if (extractedGraphDir.exists()) {
			FileUtils.deleteDirectory(extractedGraphDir);
		}

		extractedGraphDir.mkdirs();

		S3Object graphZip = s3.getObject(graphBucket, graphId+".zip");

		InputStream zipFileIn = graphZip.getObjectContent();

		OutputStream zipFileOut = new FileOutputStream(graphZipFile);

		IOUtils.copy(zipFileIn, zipFileOut);
		IOUtils.closeQuietly(zipFileIn);
		IOUtils.closeQuietly(zipFileOut);

		unpackGraphZip(graphZipFile, extractedGraphDir);
	}

	private static void unpackGraphZip(File graphZipFile, File extractedGraphDir) throws ZipException, IOException {
		// delete by default
		unpackGraphZip(graphZipFile, extractedGraphDir, true);
	}
	
	private static void unpackGraphZip(File graphZipFile, File extractedGraphDir, boolean delete) throws ZipException, IOException {
		
		ZipFile zipFile = new ZipFile(graphZipFile);
		
		Enumeration<? extends ZipEntry> entries = zipFile.entries();

		while (entries.hasMoreElements()) {

			ZipEntry entry = entries.nextElement();
			File entryDestination = new File(extractedGraphDir, entry.getName());

			entryDestination.getParentFile().mkdirs();

			if (entry.isDirectory())
				entryDestination.mkdirs();
			else {
				InputStream entryFileIn = zipFile.getInputStream(entry);
				OutputStream entryFileOut = new FileOutputStream(entryDestination);
				IOUtils.copy(entryFileIn, entryFileOut);
				IOUtils.closeQuietly(entryFileIn);
				IOUtils.closeQuietly(entryFileOut);
			}
		}

		zipFile.close();

		if (delete) {
			graphZipFile.delete();
		}
	}
	
	private static void zipGraphDir(File graphDirectory, File zipGraphFile) throws IOException {
		
		FileOutputStream fileOutputStream = new FileOutputStream(zipGraphFile);
		ZipOutputStream zipOutputStream = new ZipOutputStream(fileOutputStream);
		
		byte[] buffer = new byte[1024];
		
		for(File f : graphDirectory.listFiles()) {
			if (f.isDirectory())
				continue;
			
			ZipEntry zipEntry = new ZipEntry(f.getName());
			zipOutputStream.putNextEntry(zipEntry);
    		FileInputStream fileInput = new FileInputStream(f);

    		int len;
    		while ((len = fileInput.read(buffer)) > 0) {
    			zipOutputStream.write(buffer, 0, len);
    		}
 
    		fileInput.close();
    		zipOutputStream.closeEntry();
		}
		
		zipOutputStream.close();
	}
	

	@Override
	public int evictAll() {
		graphMap.clear();
		return 0;
	}

	@Override
	public Collection<String> getRouterIds() {
		return graphMap.keySet();
	}

	@Override
	public Factory getGraphSourceFactory() {
		return null;
	}

	@Override
	public boolean registerGraph(String arg0, GraphSource arg1) {
		return false;
	}

	@Override
	public void setDefaultRouterId(String arg0) {	
	}
}
