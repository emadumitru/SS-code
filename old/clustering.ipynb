{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\ema\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ema\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ema\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ema\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ema\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentified MEAs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, meas)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffectiveness on Test Set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, effectiveness)\n\u001b[1;32m---> 62\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     51\u001b[0m     scripts, file_names \u001b[38;5;241m=\u001b[39m load_scripts(folder_path)\n\u001b[1;32m---> 52\u001b[0m     features, feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscripts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     X_train, X_test, _, _ \u001b[38;5;241m=\u001b[39m train_test_split(features, scripts, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     55\u001b[0m     meas \u001b[38;5;241m=\u001b[39m identify_meas(X_train, feature_names)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(scripts)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(scripts):\n\u001b[0;32m     30\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)  \n\u001b[1;32m---> 31\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscripts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names()\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Directory where the edit scripts are stored\n",
    "folder_path = 'dataset\\\\Extract Method_single\\\\'\n",
    "\n",
    "# Step 1: Load and preprocess data\n",
    "def load_scripts(folder_path):\n",
    "    scripts = []\n",
    "    file_names = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):  # Assuming the scripts are in text format\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                tokens = tokenizer.tokenize(content)\n",
    "                scripts.append(' '.join(tokens))\n",
    "                file_names.append(filename)\n",
    "    \n",
    "    return scripts, file_names\n",
    "\n",
    "# Step 2 and 3: Extract features and split data\n",
    "def extract_features(scripts):\n",
    "    vectorizer = CountVectorizer(max_features=1000)  \n",
    "    X = vectorizer.fit_transform(scripts)\n",
    "    return X, vectorizer.get_feature_names()\n",
    "\n",
    "# Step 4: Identify MEAs based on the training set\n",
    "def identify_meas(features, feature_names):\n",
    "    # Simple frequency-based approach to identify MEAs\n",
    "    feature_sums = np.sum(features, axis=0)\n",
    "    sorted_indices = np.argsort(feature_sums)[::-1]\n",
    "    top_features = [feature_names[i] for i in sorted_indices[:10]]  # Top 10 MEAs\n",
    "    return top_features\n",
    "\n",
    "# Step 5: Test the MEAs on the test set\n",
    "def test_meas(features, meas, feature_names):\n",
    "    mea_indices = [feature_names.index(mea) for mea in meas if mea in feature_names]\n",
    "    test_results = features[:, mea_indices].toarray()\n",
    "    presence = np.sum(test_results, axis=1) > 0\n",
    "    return np.mean(presence)\n",
    "\n",
    "# Main function to run the script\n",
    "def main():\n",
    "    scripts, file_names = load_scripts(folder_path)\n",
    "    features, feature_names = extract_features(scripts)\n",
    "    X_train, X_test, _, _ = train_test_split(features, scripts, test_size=0.2, random_state=42)\n",
    "    \n",
    "    meas = identify_meas(X_train, feature_names)\n",
    "    effectiveness = test_meas(X_test, meas, feature_names)\n",
    "    \n",
    "    print(\"Identified MEAs:\", meas)\n",
    "    print(\"Effectiveness on Test Set:\", effectiveness)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: [{('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}, {('delete', '--')}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import difflib\n",
    "\n",
    "def parse_diff_files(path):\n",
    "    edits = []\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # This is a very basic parser, consider using GumTree or similar for real use\n",
    "    for line in lines:\n",
    "        if line.startswith('+'):\n",
    "            edits.append(('insert', line[1:].strip()))\n",
    "        elif line.startswith('-'):\n",
    "            edits.append(('delete', line[1:].strip()))\n",
    "    return edits\n",
    "\n",
    "def extract_features(edit_scripts):\n",
    "    # This function should implement the clustering logic to find MEAs\n",
    "    # Placeholder for the actual clustering and feature extraction logic\n",
    "    features = set()\n",
    "    for script in edit_scripts:\n",
    "        for edit in script:\n",
    "            features.add(edit)\n",
    "    return features\n",
    "\n",
    "def check_features_in_scripts(features, scripts):\n",
    "    results = []\n",
    "    for script in scripts:\n",
    "        script_features = set(script)\n",
    "        results.append(features.intersection(script_features))\n",
    "    return results\n",
    "\n",
    "# Splitting data into training and testing\n",
    "data_dir = 'dataset\\\\Extract Method_single\\\\'\n",
    "all_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir)]\n",
    "train_files = all_files[:70]  # ~70% for training\n",
    "test_files = all_files[70:]   # ~30% for testing\n",
    "\n",
    "# Training phase\n",
    "train_edit_scripts = [parse_diff_files(file) for file in train_files]\n",
    "train_features = extract_features(train_edit_scripts)\n",
    "\n",
    "# Testing phase\n",
    "test_edit_scripts = [parse_diff_files(file) for file in test_files]\n",
    "test_results = check_features_in_scripts(train_features, test_edit_scripts)\n",
    "\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: 78\n",
      "Testing files: 20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# List all diff files\n",
    "diff_files = [f for f in os.listdir('dataset\\\\Extract Method_single\\\\') if f.endswith('.md')]\n",
    "random.shuffle(diff_files)\n",
    "\n",
    "# Split into training and testing sets (80-20 split)\n",
    "split_index = int(0.8 * len(diff_files))\n",
    "training_files = diff_files[:split_index]\n",
    "testing_files = diff_files[split_index:]\n",
    "\n",
    "print(\"Training files:\", len(training_files))\n",
    "print(\"Testing files:\", len(testing_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common operations (MEAs): ['insert-tree', 'insert-node', 'move-tree', 'update-node', 'delete-node', 'delete-tree']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_edit_operations(file_path):\n",
    "    operations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            if any(op in line for op in ['insert', 'delete', 'update', 'move']):\n",
    "                operations.append(line.strip())\n",
    "    return operations\n",
    "\n",
    "# Collect all operations from training files\n",
    "all_operations = []\n",
    "for file_name in training_files:\n",
    "    file_path = os.path.join('dataset\\\\Extract Method_single\\\\', file_name)\n",
    "    all_operations.extend(extract_edit_operations(file_path))\n",
    "\n",
    "# Find common edit operations (minimal edit actions)\n",
    "operation_counts = Counter(all_operations)\n",
    "common_operations = [op for op, count in operation_counts.items() if count > len(training_files) * 0.5]  # Threshold at 50% occurrence\n",
    "\n",
    "print(\"Common operations (MEAs):\", common_operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: {'1123527_diff.md': True, '1116770_diff.md': True, '1122401_diff.md': True, '1118835_diff.md': True, '1117840_diff.md': True, '1121499_diff.md': True, '1121732_diff.md': True, '1120442_diff.md': True, '1122130_diff.md': True, '1120023_diff.md': True, '1123501_diff.md': True, '1111984_diff.md': True, '1116609_diff.md': True, '1110245_diff.md': True, '1119661_diff.md': True, '1117885_diff.md': True, '1102793_diff.md': True, '1120077_diff.md': True, '1121850_diff.md': True, '1121836_diff.md': True}\n"
     ]
    }
   ],
   "source": [
    "def validate_operations(test_file, common_operations):\n",
    "    test_operations = extract_edit_operations(test_file)\n",
    "    return any(op in test_operations for op in common_operations)\n",
    "\n",
    "# Test the MEAs in the testing set\n",
    "results = {}\n",
    "for file_name in testing_files:\n",
    "    file_path = os.path.join('dataset\\\\Extract Method_single\\\\', file_name)\n",
    "    results[file_name] = validate_operations(file_path, common_operations)\n",
    "\n",
    "print(\"Validation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available refactoring types:\n",
      "Extract Interface\n",
      "Extract Method\n",
      "Extract Superclass\n",
      "Inline Method\n",
      "Move Attribute\n",
      "Move Class\n",
      "Move Method\n",
      "Pull Up Attribute\n",
      "Pull Up Method\n",
      "Push Down Attribute\n",
      "Push Down Method\n",
      "Rename Package\n",
      "['move-tree', 'insert-node', 'insert-tree', 'update-node', 'delete-node', 'delete-tree']\n",
      "['move-tree', 'insert-node', 'insert-tree', 'update-node', 'delete-node', 'delete-tree']\n",
      "['move-tree', 'insert-node', 'insert-tree', 'update-node', 'delete-node', 'delete-tree']\n",
      "Threshold: 0.2, Accuracy: 1.0, Confusion Matrix:\n",
      "[[49]]\n",
      "Threshold: 0.5, Accuracy: 1.0, Confusion Matrix:\n",
      "[[49]]\n",
      "Threshold: 0.7, Accuracy: 1.0, Confusion Matrix:\n",
      "[[49]]\n",
      "Process completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract operations from diff files\n",
    "def extract_edit_operations(file_path):\n",
    "    operations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.readlines()\n",
    "        for line in data:\n",
    "            if any(op in line for op in ['insert', 'delete', 'update', 'move']):\n",
    "                operations.append(line.strip())\n",
    "    return operations\n",
    "\n",
    "# Function to validate operations in test files\n",
    "def validate_operations(test_file, common_operations):\n",
    "    test_operations = extract_edit_operations(test_file)\n",
    "    return any(op in test_operations for op in common_operations)\n",
    "\n",
    "# Set the base directory for the dataset\n",
    "dataset_dir = \"dataset_clean/\"\n",
    "\n",
    "# Gather available refactoring types\n",
    "refactoring_types = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d)) and '_single' in d]\n",
    "print(\"Available refactoring types:\")\n",
    "for ref_type in refactoring_types:\n",
    "    print(ref_type.replace('_single', ''))\n",
    "\n",
    "# User selects refactoring type for analysis\n",
    "# ref_type_to_analyze = input(\"\\nEnter the refactoring type to analyze: \") + '_single'\n",
    "ref_type_to_analyze = 'Extract Method_single'\n",
    "\n",
    "# Prepare the file lists\n",
    "file_paths = [os.path.join(dataset_dir, ref_type_to_analyze, f) for f in os.listdir(os.path.join(dataset_dir, ref_type_to_analyze)) if f.endswith('_diff.md')]\n",
    "random.shuffle(file_paths)\n",
    "\n",
    "# Split files into training and testing sets\n",
    "train_size = len(file_paths) // 2\n",
    "training_files = file_paths[:train_size]\n",
    "testing_files = file_paths[train_size:]\n",
    "\n",
    "# Collect all operations from training files\n",
    "all_operations = []\n",
    "for file_path in training_files:\n",
    "    all_operations.extend(extract_edit_operations(file_path))\n",
    "\n",
    "# Determine common operations at different thresholds\n",
    "thresholds = [0.2, 0.5, 0.7]\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    operation_counts = Counter(all_operations)\n",
    "    common_operations = [op for op, count in operation_counts.items() if count >= len(training_files) * threshold]\n",
    "\n",
    "    print(common_operations)\n",
    "\n",
    "    # Test operations in the testing set\n",
    "    y_true = [1] * len(testing_files)  # All are positive examples\n",
    "    y_pred = [validate_operations(f, common_operations) for f in testing_files]\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    results.append((threshold, accuracy, cm))\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Threshold: {res[0]}, Accuracy: {res[1]}, Confusion Matrix:\\n{res[2]}\")\n",
    "\n",
    "print(\"Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Extract Interface_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        60\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.50      0.49      0.50        60\n",
      "weighted avg       1.00      0.98      0.99        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Extract Interface_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Classification report for Extract Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.92        60\n",
      "   macro avg       0.48      0.47      0.48        60\n",
      "weighted avg       0.93      0.92      0.92        60\n",
      "\n",
      "Classification report for Extract Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.38      0.51        32\n",
      "           1       0.56      0.89      0.68        28\n",
      "\n",
      "    accuracy                           0.62        60\n",
      "   macro avg       0.68      0.63      0.60        60\n",
      "weighted avg       0.69      0.62      0.59        60\n",
      "\n",
      "Classification report for Extract Superclass_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.48      0.50      0.49        60\n",
      "weighted avg       0.93      0.97      0.95        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Extract Superclass_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Inline Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Inline Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        54\n",
      "           1       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.90        60\n",
      "   macro avg       0.45      0.50      0.47        60\n",
      "weighted avg       0.81      0.90      0.85        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Move Attribute_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.49      0.49      0.49        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Classification report for Move Attribute_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.48      0.50      0.49        60\n",
      "weighted avg       0.93      0.97      0.95        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Move Class_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.48      0.48      0.48        60\n",
      "weighted avg       0.93      0.93      0.93        60\n",
      "\n",
      "Classification report for Move Class_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93        52\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.43      0.50      0.46        60\n",
      "weighted avg       0.75      0.87      0.80        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Move Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.48      0.48      0.48        60\n",
      "weighted avg       0.93      0.93      0.93        60\n",
      "\n",
      "Classification report for Move Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Pull Up Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Classification report for Pull Up Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.48      0.50      0.49        60\n",
      "weighted avg       0.93      0.97      0.95        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Rename Package_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Rename Package_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "classify_new_script() missing 1 required positional argument: 'vectorizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m new_script_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mExtract Method_mixed\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m1107345_diff.md\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref_type \u001b[38;5;129;01min\u001b[39;00m refactoring_types:\n\u001b[1;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_new_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_script_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs the new script \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: classify_new_script() missing 1 required positional argument: 'vectorizer'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_and_labels(data_folder):\n",
    "    # Load all files and their labels\n",
    "    refactoring_types = [d for d in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, d))]\n",
    "    documents = []\n",
    "    labels = []\n",
    "\n",
    "    for ref_type in refactoring_types:\n",
    "        subfolder_path = os.path.join(data_folder, ref_type)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith('.md'):\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "                    documents.append(content)\n",
    "                    labels.append(ref_type)\n",
    "    return documents, labels, refactoring_types\n",
    "\n",
    "def extract_features(documents):\n",
    "    # Fit the CountVectorizer to all documents to create a global vocabulary\n",
    "    vectorizer = CountVectorizer()\n",
    "    features = vectorizer.fit_transform(documents).toarray()\n",
    "    return features, vectorizer.get_feature_names_out()\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "documents, labels, refactoring_types = load_data_and_labels(data_folder)\n",
    "features, feature_names = extract_features(documents)\n",
    "\n",
    "# Convert labels to a binary format for each refactoring type\n",
    "classifiers = {}\n",
    "for ref_type in refactoring_types:\n",
    "    binary_labels = [1 if label == ref_type else 0 for label in labels]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, binary_labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train a RandomForest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    classifiers[ref_type] = clf\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(f\"Classification report for {ref_type}:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# Function to classify new scripts\n",
    "def classify_new_script(script_path, ref_type, vectorizer):\n",
    "    with open(script_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    file_features = vectorizer.transform([content]).toarray()\n",
    "    clf = classifiers[ref_type]\n",
    "    prediction = clf.predict(file_features)\n",
    "    return bool(prediction[0])\n",
    "\n",
    "# Example usage\n",
    "new_script_path = \"dataset\\\\Extract Method_mixed\\\\1107345_diff.md\"\n",
    "for ref_type in refactoring_types:\n",
    "    result = classify_new_script(new_script_path, ref_type)\n",
    "    print(f\"Is the new script {ref_type}? {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Extract Interface_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        60\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.50      0.49      0.50        60\n",
      "weighted avg       1.00      0.98      0.99        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Extract Interface_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Classification report for Extract Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.92        60\n",
      "   macro avg       0.48      0.47      0.48        60\n",
      "weighted avg       0.93      0.92      0.92        60\n",
      "\n",
      "Classification report for Extract Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.41      0.54        32\n",
      "           1       0.57      0.89      0.69        28\n",
      "\n",
      "    accuracy                           0.63        60\n",
      "   macro avg       0.69      0.65      0.62        60\n",
      "weighted avg       0.70      0.63      0.61        60\n",
      "\n",
      "Classification report for Extract Superclass_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.48      0.50      0.49        60\n",
      "weighted avg       0.93      0.97      0.95        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Extract Superclass_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Inline Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Inline Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        54\n",
      "           1       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.90        60\n",
      "   macro avg       0.45      0.50      0.47        60\n",
      "weighted avg       0.81      0.90      0.85        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Move Attribute_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.49      0.49      0.49        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "Classification report for Move Attribute_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.48      0.50      0.49        60\n",
      "weighted avg       0.93      0.97      0.95        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Move Class_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.48      0.48      0.48        60\n",
      "weighted avg       0.93      0.93      0.93        60\n",
      "\n",
      "Classification report for Move Class_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93        52\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.43      0.50      0.46        60\n",
      "weighted avg       0.75      0.87      0.80        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Move Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.48      0.48      0.48        60\n",
      "weighted avg       0.93      0.93      0.93        60\n",
      "\n",
      "Classification report for Move Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Pull Up Method_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Classification report for Pull Up Method_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        58\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.48      0.50      0.49        60\n",
      "weighted avg       0.93      0.97      0.95        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Rename Package_mixed:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Rename Package_single:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        59\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98        60\n",
      "   macro avg       0.49      0.50      0.50        60\n",
      "weighted avg       0.97      0.98      0.98        60\n",
      "\n",
      "Is the new script Extract Interface_mixed? False\n",
      "Is the new script Extract Interface_single? False\n",
      "Is the new script Extract Method_mixed? True\n",
      "Is the new script Extract Method_single? False\n",
      "Is the new script Extract Superclass_mixed? False\n",
      "Is the new script Extract Superclass_single? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the new script Inline Method_mixed? False\n",
      "Is the new script Inline Method_single? False\n",
      "Is the new script Move Attribute_mixed? False\n",
      "Is the new script Move Attribute_single? False\n",
      "Is the new script Move Class_mixed? False\n",
      "Is the new script Move Class_single? False\n",
      "Is the new script Move Method_mixed? False\n",
      "Is the new script Move Method_single? False\n",
      "Is the new script Pull Up Method_mixed? False\n",
      "Is the new script Pull Up Method_single? False\n",
      "Is the new script Rename Package_mixed? False\n",
      "Is the new script Rename Package_single? False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_and_labels(data_folder):\n",
    "    # Load all files and their labels\n",
    "    refactoring_types = [d for d in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, d))]\n",
    "    documents = []\n",
    "    labels = []\n",
    "\n",
    "    for ref_type in refactoring_types:\n",
    "        subfolder_path = os.path.join(data_folder, ref_type)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            if filename.endswith('.md'):\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "                    documents.append(content)\n",
    "                    labels.append(ref_type)\n",
    "    return documents, labels, refactoring_types\n",
    "\n",
    "def extract_features(documents):\n",
    "    # Fit the CountVectorizer to all documents to create a global vocabulary\n",
    "    vectorizer = CountVectorizer()\n",
    "    features = vectorizer.fit_transform(documents).toarray()\n",
    "    return features, vectorizer\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "documents, labels, refactoring_types = load_data_and_labels(data_folder)\n",
    "features, vectorizer = extract_features(documents)\n",
    "\n",
    "# Convert labels to a binary format for each refactoring type\n",
    "classifiers = {}\n",
    "for ref_type in refactoring_types:\n",
    "    binary_labels = [1 if label == ref_type else 0 for label in labels]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, binary_labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train a RandomForest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    classifiers[ref_type] = clf\n",
    "    \n",
    "    # Evaluate the classifier\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(f\"Classification report for {ref_type}:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# Function to classify new scripts\n",
    "def classify_new_script(script_path, ref_type, vectorizer):\n",
    "    with open(script_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    file_features = vectorizer.transform([content]).toarray()\n",
    "    clf = classifiers[ref_type]\n",
    "    prediction = clf.predict(file_features)\n",
    "    return bool(prediction[0])\n",
    "\n",
    "# Example usage\n",
    "new_script_path = \"dataset\\\\Extract Method_mixed\\\\1107345_diff.md\"  # This should be replaced by the actual path you want to use.\n",
    "for ref_type in refactoring_types:\n",
    "    result = classify_new_script(new_script_path, ref_type, vectorizer)\n",
    "    print(f\"Is the new script {ref_type}? {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ema\\AppData\\Local\\Temp\\ipykernel_30420\\942048272.py:26: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  is_positive = float(ref_details.loc[ref_details['id'] == int(file_id), ref_type_clean]) > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        True       1.00      1.00      1.00        60\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "Is the new script a specified refactoring type? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data_and_labels(data_folder, refactoring_details_path):\n",
    "    ref_details = pd.read_csv(refactoring_details_path)\n",
    "    documents = []\n",
    "    labels = []\n",
    "\n",
    "    # Load documents and initially label them based on folder name\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        subfolder_path = os.path.join(data_folder, ref_type)\n",
    "        for filename in os.listdir(subfolder_path):\n",
    "            file_id = filename.split('_')[0]\n",
    "            if filename.endswith('.md'):\n",
    "                file_path = os.path.join(subfolder_path, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "                    documents.append(content)\n",
    "                    # Determine label from CSV, 1 if positive, 0 if negative for the ref_type without '_single' or '_mixed'\n",
    "                    ref_type_clean = ref_type.split('_')[0]\n",
    "                    is_positive = float(ref_details.loc[ref_details['id'] == int(file_id), ref_type_clean]) > 0\n",
    "                    labels.append(is_positive)\n",
    "    return documents, labels\n",
    "\n",
    "def extract_features(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    features = vectorizer.fit_transform(documents).toarray()\n",
    "    return features, vectorizer\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "refactoring_details_path = 'analysis_results\\\\refactoring_details_post.csv'\n",
    "documents, labels = load_data_and_labels(data_folder, refactoring_details_path)\n",
    "features, vectorizer = extract_features(documents)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "def classify_new_script(script_path, vectorizer):\n",
    "    with open(script_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    file_features = vectorizer.transform([content]).toarray()\n",
    "    prediction = clf.predict(file_features)\n",
    "    return bool(prediction[0])\n",
    "\n",
    "new_script_path = \"dataset\\\\Extract Method_mixed\\\\1107345_diff.md\"  # Update with actual path to new script\n",
    "result = classify_new_script(new_script_path, vectorizer)\n",
    "print(f\"Is the new script a specified refactoring type? {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [157, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m refactoring_details_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis_results\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mrefactoring_details_post.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m documents, training_labels, testing_labels \u001b[38;5;241m=\u001b[39m load_data(data_folder, refactoring_details_path)\n\u001b[1;32m---> 84\u001b[0m classifiers, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m refactoring_type, report \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[23], line 66\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(documents, training_labels, testing_labels)\u001b[0m\n\u001b[0;32m     64\u001b[0m y_train \u001b[38;5;241m=\u001b[39m labels\n\u001b[0;32m     65\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m---> 66\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m classifiers[refactoring_type] \u001b[38;5;241m=\u001b[39m clf\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Prepare testing data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1281\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [157, 2]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data(data_folder, refactoring_details_path):\n",
    "    ref_details = pd.read_csv(refactoring_details_path)\n",
    "    documents = []\n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "    testing_documents = []\n",
    "\n",
    "    # Load training data\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_single' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_single', '')\n",
    "            training_labels[ref_type_clean] = []\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        documents.append(content)\n",
    "                        training_labels[ref_type_clean].append(1)  # All files in _single are true examples\n",
    "\n",
    "    # Load testing data\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_mixed' in ref_type:\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_id = filename.split('_')[0]\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        testing_documents.append((file_id, content))\n",
    "    \n",
    "    # Assign labels based on CSV for testing data\n",
    "    for file_id, content in testing_documents:\n",
    "        for refactoring_type in training_labels.keys():\n",
    "            count = ref_details.loc[ref_details['id'] == int(file_id), refactoring_type].values[0]\n",
    "            if count > 1:\n",
    "                if refactoring_type not in testing_labels:\n",
    "                    testing_labels[refactoring_type] = []\n",
    "                testing_labels[refactoring_type].append((content, 1))\n",
    "            elif count <= 1:\n",
    "                if refactoring_type not in testing_labels:\n",
    "                    testing_labels[refactoring_type] = []\n",
    "                testing_labels[refactoring_type].append((content, 0))\n",
    "    \n",
    "    return documents, training_labels, testing_labels\n",
    "\n",
    "def train_and_evaluate(documents, training_labels, testing_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    classifiers = {}\n",
    "    results = {}\n",
    "    \n",
    "    for refactoring_type, labels in training_labels.items():\n",
    "        y_train = labels\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        classifiers[refactoring_type] = clf\n",
    "\n",
    "        # Prepare testing data\n",
    "        X_test = [vectorizer.transform([doc]) for doc, _ in testing_labels[refactoring_type]]\n",
    "        y_test = [label for _, label in testing_labels[refactoring_type]]\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Evaluate classifier\n",
    "        predictions = clf.predict(X_test)\n",
    "        report = classification_report(y_test, predictions, target_names=['False', 'True'])\n",
    "        results[refactoring_type] = report\n",
    "\n",
    "    return classifiers, results\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "refactoring_details_path = 'analysis_results\\\\refactoring_details_post.csv'\n",
    "documents, training_labels, testing_labels = load_data(data_folder, refactoring_details_path)\n",
    "classifiers, results = train_and_evaluate(documents, training_labels, testing_labels)\n",
    "\n",
    "# Display results\n",
    "for refactoring_type, report in results.items():\n",
    "    print(f\"Classification report for {refactoring_type}:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "{'Extract Interface': [1, 1], 'Extract Method': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'Extract Superclass': [1, 1, 1, 1, 1], 'Inline Method': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'Move Attribute': [1, 1, 1, 1, 1, 1, 1], 'Move Class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'Move Method': [1, 1, 1, 1, 1], 'Pull Up Method': [1, 1, 1, 1], 'Rename Package': [1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data(data_folder, refactoring_details_path):\n",
    "    ref_details = pd.read_csv(refactoring_details_path)\n",
    "    documents = []\n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "    testing_documents = []\n",
    "\n",
    "    # Load training data\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_single' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_single', '')\n",
    "            training_labels[ref_type_clean] = []\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        documents.append(content)\n",
    "                        training_labels[ref_type_clean].append(1)  # All files in _single are true examples\n",
    "\n",
    "    # Load testing data\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_mixed' in ref_type:\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_id = filename.split('_')[0]\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        testing_documents.append((file_id, content))\n",
    "    \n",
    "    # Assign labels based on CSV for testing data\n",
    "    for file_id, content in testing_documents:\n",
    "        for refactoring_type in training_labels.keys():\n",
    "            count = ref_details.loc[ref_details['id'] == int(file_id), refactoring_type].values[0]\n",
    "            if count > 1:\n",
    "                if refactoring_type not in testing_labels:\n",
    "                    testing_labels[refactoring_type] = []\n",
    "                testing_labels[refactoring_type].append((content, 1))\n",
    "            elif count <= 1:\n",
    "                if refactoring_type not in testing_labels:\n",
    "                    testing_labels[refactoring_type] = []\n",
    "                testing_labels[refactoring_type].append((content, 0))\n",
    "    \n",
    "    return documents, training_labels, testing_labels\n",
    "\n",
    "def train_and_evaluate(documents, training_labels, testing_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    classifiers = {}\n",
    "    results = {}\n",
    "    \n",
    "    for refactoring_type, labels in training_labels.items():\n",
    "        y_train = labels\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        classifiers[refactoring_type] = clf\n",
    "\n",
    "        # Prepare testing data\n",
    "        X_test = [vectorizer.transform([doc]) for doc, _ in testing_labels[refactoring_type]]\n",
    "        y_test = [label for _, label in testing_labels[refactoring_type]]\n",
    "        X_test = np.vstack(X_test)\n",
    "\n",
    "        # Evaluate classifier\n",
    "        predictions = clf.predict(X_test)\n",
    "        report = classification_report(y_test, predictions, target_names=['False', 'True'])\n",
    "        results[refactoring_type] = report\n",
    "\n",
    "    return classifiers, results\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "refactoring_details_path = 'analysis_results\\\\refactoring_details_post.csv'\n",
    "documents, training_labels, testing_labels = load_data(data_folder, refactoring_details_path)\n",
    "print('labels')\n",
    "print(training_labels)\n",
    "classifiers, results = train_and_evaluate(documents, training_labels, testing_labels)\n",
    "\n",
    "# Display results\n",
    "for refactoring_type, report in results.items():\n",
    "    print(f\"Classification report for {refactoring_type}:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Extract Interface': [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Extract Method': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Extract Superclass': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Inline Method': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Move Attribute': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Move Class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Move Method': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'Pull Up Method': [1, 1, 1, 1, 0, 0, 0, 0], 'Rename Package': [1, 1, 1, 1]}\n",
      "{'Extract Interface': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Extract Method': [1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Extract Superclass': [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Inline Method': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Move Attribute': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], 'Move Class': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], 'Move Method': [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'Pull Up Method': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0], 'Rename Package': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_labels)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_labels)\n\u001b[1;32m---> 87\u001b[0m classifiers, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m refactoring_type, report \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[15], line 77\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train_docs, train_labels, test_docs, test_labels)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predictions)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(test_labels[refactoring_type])\n\u001b[1;32m---> 77\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrefactoring_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFalse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     results[refactoring_type] \u001b[38;5;241m=\u001b[39m report\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classifiers, results\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ema\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2626\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2620\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2621\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2622\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[0;32m   2623\u001b[0m             )\n\u001b[0;32m   2624\u001b[0m         )\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2626\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2627\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, does not match size of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2628\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. Try specifying the labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[0;32m   2630\u001b[0m         )\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2632\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes, 1, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data(data_folder, refactoring_details_path):\n",
    "    ref_details = pd.read_csv(refactoring_details_path)\n",
    "    train_documents = []\n",
    "    test_documents = []\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "\n",
    "    # Load training data from _single directories\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_single' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_single', '')\n",
    "            training_labels[ref_type_clean] = []\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    file_id = filename.split('_')[0]\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        train_documents.append(content)\n",
    "                        train_ids.append(file_id)\n",
    "                        # Initialize labels for all refactoring types\n",
    "                        for key in training_labels:\n",
    "                            training_labels[key].append(0)\n",
    "                        training_labels[ref_type_clean][-1] = 1  # Set the correct label for this refactoring type\n",
    "\n",
    "    # Load testing data from _mixed directories\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_mixed' in ref_type:\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    file_id = filename.split('_')[0]\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        test_documents.append(content)\n",
    "                        test_ids.append(file_id)\n",
    "\n",
    "    # Assign labels for testing based on CSV data\n",
    "    for idx, doc in enumerate(test_documents):\n",
    "        file_id = test_ids[idx]\n",
    "        for refactoring_type in training_labels.keys():\n",
    "            count = ref_details.loc[ref_details['id'] == int(file_id), refactoring_type].values[0]\n",
    "            if refactoring_type not in testing_labels:\n",
    "                testing_labels[refactoring_type] = []\n",
    "            testing_labels[refactoring_type].append(1 if count > 1 else 0)\n",
    "    \n",
    "    return train_documents, train_ids, test_documents, test_ids, training_labels, testing_labels\n",
    "\n",
    "def train_and_evaluate(train_docs, train_labels, test_docs, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_docs)\n",
    "    X_test = vectorizer.transform(test_docs)\n",
    "    \n",
    "    classifiers = {}\n",
    "    results = {}\n",
    "    \n",
    "    for refactoring_type, labels in train_labels.items():\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, labels)\n",
    "        classifiers[refactoring_type] = clf\n",
    "\n",
    "        # Evaluate classifier\n",
    "        predictions = clf.predict(X_test)\n",
    "        report = classification_report(test_labels[refactoring_type], predictions, target_names=['False', 'True'])\n",
    "        results[refactoring_type] = report\n",
    "\n",
    "    return classifiers, results\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "refactoring_details_path = 'analysis_results\\\\refactoring_details_post.csv'\n",
    "train_docs, train_ids, test_docs, test_ids, train_labels, test_labels = load_data(data_folder, refactoring_details_path)\n",
    "print(train_labels)\n",
    "print(test_labels)\n",
    "classifiers, results = train_and_evaluate(train_docs, train_labels, test_docs, test_labels)\n",
    "\n",
    "# Display results\n",
    "for refactoring_type, report in results.items():\n",
    "    print(f\"Classification report for {refactoring_type}:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No positive predictions for Extract Interface. All predictions are negative.\n",
      "No positive predictions for Extract Superclass. All predictions are negative.\n",
      "No positive predictions for Inline Method. All predictions are negative.\n",
      "No positive predictions for Move Attribute. All predictions are negative.\n",
      "No positive predictions for Move Class. All predictions are negative.\n",
      "No positive predictions for Move Method. All predictions are negative.\n",
      "No positive predictions for Pull Up Method. All predictions are negative.\n",
      "No positive predictions for Rename Package. All predictions are negative.\n",
      "Results for Extract Interface:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      1.00      0.94        37\n",
      "        True       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.44      0.50      0.47        42\n",
      "weighted avg       0.78      0.88      0.83        42\n",
      "\n",
      "Results for Extract Method:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.47      0.32      0.38        22\n",
      "        True       0.44      0.60      0.51        20\n",
      "\n",
      "    accuracy                           0.45        42\n",
      "   macro avg       0.46      0.46      0.44        42\n",
      "weighted avg       0.46      0.45      0.44        42\n",
      "\n",
      "Results for Extract Superclass:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      1.00      0.92        36\n",
      "        True       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.43      0.50      0.46        42\n",
      "weighted avg       0.73      0.86      0.79        42\n",
      "\n",
      "Results for Inline Method:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      1.00      0.94        37\n",
      "        True       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.44      0.50      0.47        42\n",
      "weighted avg       0.78      0.88      0.83        42\n",
      "\n",
      "Results for Move Attribute:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      1.00      0.85        31\n",
      "        True       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.37      0.50      0.42        42\n",
      "weighted avg       0.54      0.74      0.63        42\n",
      "\n",
      "Results for Move Class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      1.00      0.71        23\n",
      "        True       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.55        42\n",
      "   macro avg       0.27      0.50      0.35        42\n",
      "weighted avg       0.30      0.55      0.39        42\n",
      "\n",
      "Results for Move Method:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      1.00      0.85        31\n",
      "        True       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.37      0.50      0.42        42\n",
      "weighted avg       0.54      0.74      0.63        42\n",
      "\n",
      "Results for Pull Up Method:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      1.00      0.92        36\n",
      "        True       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.43      0.50      0.46        42\n",
      "weighted avg       0.73      0.86      0.79        42\n",
      "\n",
      "Results for Rename Package:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      1.00      0.91        35\n",
      "        True       0.00      0.00      0.00         7\n",
      "\n",
      "    accuracy                           0.83        42\n",
      "   macro avg       0.42      0.50      0.45        42\n",
      "weighted avg       0.69      0.83      0.76        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data(data_folder, refactoring_details_path):\n",
    "    ref_details = pd.read_csv(refactoring_details_path)\n",
    "    train_documents = []\n",
    "    test_documents = []\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "\n",
    "    # Initialize training labels for all refactoring types\n",
    "    ref_types = [d.replace('_single', '') for d in os.listdir(data_folder) if '_single' in d]\n",
    "    for ref_type in ref_types:\n",
    "        training_labels[ref_type] = []\n",
    "        testing_labels[ref_type] = []\n",
    "\n",
    "    # Load training data from _single directories and assign labels\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_single' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_single', '')\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        train_documents.append(content)\n",
    "                        train_ids.append(filename.split('_')[0])\n",
    "                        # Initialize labels for all refactoring types as 0\n",
    "                        for key in training_labels:\n",
    "                            training_labels[key].append(0)\n",
    "                        # Set the current refactoring type label to 1\n",
    "                        training_labels[ref_type_clean][-1] = 1\n",
    "\n",
    "    # Load testing data from _mixed directories and assign labels based on CSV\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_mixed' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_mixed', '')\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        test_documents.append(content)\n",
    "                        test_ids.append(filename.split('_')[0])\n",
    "                        file_id = filename.split('_')[0]\n",
    "                        # Initialize labels for all refactoring types as 0\n",
    "                        for key in testing_labels:\n",
    "                            testing_labels[key].append(0)\n",
    "                        # Set labels based on occurrence counts from the CSV\n",
    "                        for key in ref_types:\n",
    "                            count = ref_details.loc[ref_details['id'] == int(file_id), key].values[0]\n",
    "                            if count > 0:\n",
    "                                testing_labels[key][-1] = 1\n",
    "\n",
    "    return train_documents, train_ids, test_documents, test_ids, training_labels, testing_labels\n",
    "\n",
    "def train_and_evaluate(train_docs, train_labels, test_docs, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_docs)\n",
    "    X_test = vectorizer.transform(test_docs)\n",
    "    \n",
    "    classifiers = {}\n",
    "    results = {}\n",
    "    \n",
    "    for refactoring_type, labels in train_labels.items():\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, labels)\n",
    "        classifiers[refactoring_type] = clf\n",
    "\n",
    "        # Evaluate classifier\n",
    "        predictions = clf.predict(X_test)\n",
    "        actual_labels = np.array(test_labels[refactoring_type])\n",
    "        \n",
    "        # Handle cases where only one class is present in the test labels\n",
    "        if len(set(actual_labels)) == 1:\n",
    "            print(f\"Only one class present for {refactoring_type}. Cannot generate a full classification report.\")\n",
    "            cm = confusion_matrix(actual_labels, predictions, labels=[0, 1])\n",
    "            accuracy = np.mean(predictions == actual_labels)\n",
    "            results[refactoring_type] = {'confusion_matrix': cm, 'accuracy': accuracy}\n",
    "        elif len(set(predictions)) == 1 and 1 not in predictions:\n",
    "            print(f\"No positive predictions for {refactoring_type}. All predictions are negative.\")\n",
    "            report = classification_report(test_labels[refactoring_type], predictions, target_names=['False', 'True'], zero_division=0)\n",
    "            results[refactoring_type] = report\n",
    "        else:\n",
    "            report = classification_report(actual_labels, predictions, target_names=['False', 'True'])\n",
    "            results[refactoring_type] = report\n",
    "\n",
    "    return classifiers, results\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "refactoring_details_path = 'analysis_results\\\\refactoring_details_post.csv'\n",
    "train_docs, train_ids, test_docs, test_ids, train_labels, test_labels = load_data(data_folder, refactoring_details_path)\n",
    "classifiers, results = train_and_evaluate(train_docs, train_labels, test_docs, test_labels)\n",
    "\n",
    "# Display results\n",
    "for refactoring_type, result in results.items():\n",
    "    print(f\"Results for {refactoring_type}:\")\n",
    "    if isinstance(result, dict) and 'confusion_matrix' in result:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(result['confusion_matrix'])\n",
    "        print(\"Accuracy:\", result['accuracy'])\n",
    "    else:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Extract Interface:\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 5  0]]\n",
      "Accuracy: 0.8809523809523809\n",
      "Results for Extract Method:\n",
      "Confusion Matrix:\n",
      "[[ 7 15]\n",
      " [ 8 12]]\n",
      "Accuracy: 0.4523809523809524\n",
      "Results for Extract Superclass:\n",
      "Confusion Matrix:\n",
      "[[36  0]\n",
      " [ 6  0]]\n",
      "Accuracy: 0.8571428571428571\n",
      "Results for Inline Method:\n",
      "Confusion Matrix:\n",
      "[[37  0]\n",
      " [ 5  0]]\n",
      "Accuracy: 0.8809523809523809\n",
      "Results for Move Attribute:\n",
      "Confusion Matrix:\n",
      "[[31  0]\n",
      " [11  0]]\n",
      "Accuracy: 0.7380952380952381\n",
      "Results for Move Class:\n",
      "Confusion Matrix:\n",
      "[[23  0]\n",
      " [19  0]]\n",
      "Accuracy: 0.5476190476190477\n",
      "Results for Move Method:\n",
      "Confusion Matrix:\n",
      "[[31  0]\n",
      " [11  0]]\n",
      "Accuracy: 0.7380952380952381\n",
      "Results for Pull Up Method:\n",
      "Confusion Matrix:\n",
      "[[36  0]\n",
      " [ 6  0]]\n",
      "Accuracy: 0.8571428571428571\n",
      "Results for Rename Package:\n",
      "Confusion Matrix:\n",
      "[[35  0]\n",
      " [ 7  0]]\n",
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "def load_data(data_folder, refactoring_details_path):\n",
    "    ref_details = pd.read_csv(refactoring_details_path)\n",
    "    train_documents = []\n",
    "    test_documents = []\n",
    "    train_ids = []\n",
    "    test_ids = []\n",
    "    training_labels = {}\n",
    "    testing_labels = {}\n",
    "\n",
    "    # Initialize training labels for all refactoring types\n",
    "    ref_types = [d.replace('_single', '') for d in os.listdir(data_folder) if '_single' in d]\n",
    "    for ref_type in ref_types:\n",
    "        training_labels[ref_type] = []\n",
    "        testing_labels[ref_type] = []\n",
    "\n",
    "    # Load training data from _single directories and assign labels\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_single' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_single', '')\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        train_documents.append(content)\n",
    "                        train_ids.append(filename.split('_')[0])\n",
    "                        # Initialize labels for all refactoring types as 0\n",
    "                        for key in training_labels:\n",
    "                            training_labels[key].append(0)\n",
    "                        # Set the current refactoring type label to 1\n",
    "                        training_labels[ref_type_clean][-1] = 1\n",
    "\n",
    "    # Load testing data from _mixed directories and assign labels based on CSV\n",
    "    for ref_type in os.listdir(data_folder):\n",
    "        if '_mixed' in ref_type:\n",
    "            ref_type_clean = ref_type.replace('_mixed', '')\n",
    "            subfolder_path = os.path.join(data_folder, ref_type)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.endswith('.md'):\n",
    "                    file_path = os.path.join(subfolder_path, filename)\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read()\n",
    "                        test_documents.append(content)\n",
    "                        test_ids.append(filename.split('_')[0])\n",
    "                        file_id = filename.split('_')[0]\n",
    "                        # Initialize labels for all refactoring types as 0\n",
    "                        for key in testing_labels:\n",
    "                            testing_labels[key].append(0)\n",
    "                        # Set labels based on occurrence counts from the CSV\n",
    "                        for key in ref_types:\n",
    "                            count = ref_details.loc[ref_details['id'] == int(file_id), key].values[0]\n",
    "                            if count > 0:\n",
    "                                testing_labels[key][-1] = 1\n",
    "\n",
    "    return train_documents, train_ids, test_documents, test_ids, training_labels, testing_labels\n",
    "\n",
    "def train_and_evaluate(train_docs, train_labels, test_docs, test_labels, path_save='analysis_results\\\\'):\n",
    "    vectorizer=CountVectorizer()\n",
    "    X_train = vectorizer.fit_transform(train_docs)\n",
    "    X_test = vectorizer.transform(test_docs)\n",
    "    classifiers = {}\n",
    "    results = {}\n",
    "    summary = {}\n",
    "    \n",
    "    for refactoring_type, labels in train_labels.items():\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, labels)\n",
    "        classifiers[refactoring_type] = clf\n",
    "\n",
    "        # Evaluate classifier\n",
    "        predictions = clf.predict(X_test)\n",
    "        cm = confusion_matrix(test_labels[refactoring_type], predictions)\n",
    "        acc = accuracy_score(test_labels[refactoring_type], predictions)\n",
    "        \n",
    "        # Store results\n",
    "        results[refactoring_type] = {\n",
    "            'confusion_matrix': cm,\n",
    "            'accuracy': acc\n",
    "        }\n",
    "        summary[refactoring_type] = [acc, *cm.ravel()]  # Flatten confusion matrix and prepend accuracy\n",
    "\n",
    "    # Serialize results and summary\n",
    "    with open(path_save + 'classification_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    with open(path_save + 'classification_summary.pkl', 'wb') as f:\n",
    "        pickle.dump(summary, f)\n",
    "    \n",
    "    return classifiers, results\n",
    "\n",
    "data_folder = \"dataset_clean\\\\\"\n",
    "refactoring_details_path = 'analysis_results\\\\refactoring_details_post.csv'\n",
    "train_docs, train_ids, test_docs, test_ids, train_labels, test_labels = load_data(data_folder, refactoring_details_path)\n",
    "classifiers, results = train_and_evaluate(train_docs, train_labels, test_docs, test_labels)\n",
    "\n",
    "# Display results\n",
    "for refactoring_type, result in results.items():\n",
    "    print(f\"Results for {refactoring_type}:\")\n",
    "    if isinstance(result, dict) and 'confusion_matrix' in result:\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(result['confusion_matrix'])\n",
    "        print(\"Accuracy:\", result['accuracy'])\n",
    "    else:\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
